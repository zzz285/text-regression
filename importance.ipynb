{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = set(['.', ',', '!', '?', ';', ':', '\"', \"'\", '(', ')', '[', ']', '{', '}']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "same=pd.read_csv('word importance/2000-2003.csv') \n",
    "# this table contains the stock volatility and company index in these years\n",
    "data_index1=same['2000'].tolist()\n",
    "data_index2=same['2001'].tolist()\n",
    "data_index3=same['2002'].tolist()\n",
    "data_index4=same['2003'].tolist()\n",
    "trainY=same['2000-12'].tolist()+same['2001-12'].tolist()+same['2002-12'].tolist()+same['2003-12'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=[]\n",
    "# get the tokenized text\n",
    "folder_path1='token/2000'\n",
    "folder_path2='token/2001'\n",
    "folder_path3='token/2002'\n",
    "folder_path4='token/2003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in data_index1:\n",
    "    file_path = os.path.join(folder_path1, file_name+'.mda')\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            trainX.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in data_index2:\n",
    "    file_path = os.path.join(folder_path2, file_name+'.mda')\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            trainX.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in data_index3:\n",
    "    file_path = os.path.join(folder_path3, file_name+'.mda')\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            trainX.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in data_index4:\n",
    "    file_path = os.path.join(folder_path4, file_name+'.mda')\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            trainX.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4492, 4492)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX),len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'  \n",
    "batch_size = 16                   \n",
    "max_length = 128                 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "x = trainX\n",
    "y = trainY\n",
    "\n",
    "word_correlations = defaultdict(lambda: {'scores': [], 'ys': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 281/281 [03:21<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx in tqdm(range(0, len(x), batch_size), desc=\"Processing\"):\n",
    "    batch_texts = x[batch_idx:batch_idx+batch_size]\n",
    "    batch_labels = y[batch_idx:batch_idx+batch_size]\n",
    "    \n",
    "    preprocessed_texts = [preprocess_text(text) for text in batch_texts]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        preprocessed_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    # offset_mapping is needed to map tokens to words\n",
    "    offset_mapping = inputs.pop('offset_mapping').cpu().numpy() \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        attentions = [a.cpu() for a in outputs.attentions]\n",
    "    del outputs\n",
    "    \n",
    "    # get the last layer attention and average over heads\n",
    "    last_layer_att = attentions[-1].mean(1).mean(1).numpy() \n",
    "    \n",
    "    # process each sample in the batch\n",
    "    for i in range(len(batch_texts)):\n",
    "        text = preprocessed_texts[i]\n",
    "        original_text = batch_texts[i]  \n",
    "       \n",
    "        words = text.split()\n",
    "      \n",
    "        word_offsets = []\n",
    "        current_pos = 0\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                current_pos += len(word) + 1  \n",
    "                continue\n",
    "            start = current_pos\n",
    "            end = current_pos + len(word)\n",
    "            word_offsets.append( (start, end) )\n",
    "            current_pos = end + 1\n",
    "\n",
    "        valid_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "        sample_offsets = offset_mapping[i]\n",
    "        sample_scores = last_layer_att[i]\n",
    "        word_scores = [[] for _ in valid_words]\n",
    "        \n",
    "        # map tokens to words\n",
    "        for token_idx, (start, end) in enumerate(sample_offsets):\n",
    "            token_text = tokenizer.decode(inputs['input_ids'][i][token_idx])\n",
    "            if token_text in punctuations:\n",
    "                continue\n",
    "            if start == end == 0:\n",
    "                continue\n",
    "            # find the word that the token belongs to\n",
    "            for word_idx, (word_start, word_end) in enumerate(word_offsets):\n",
    "                if start >= word_start and end <= word_end:\n",
    "                    word_scores[word_idx].append(sample_scores[token_idx])\n",
    "                    break\n",
    "        # calculate the average score for each word\n",
    "        for word_idx, scores in enumerate(word_scores):\n",
    "            if not scores:\n",
    "                continue\n",
    "            word = valid_words[word_idx]\n",
    "            avg_score = np.mean(scores)\n",
    "            word_correlations[word]['scores'].append(avg_score)\n",
    "            word_correlations[word]['ys'].append(batch_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zzz\\AppData\\Local\\Temp\\ipykernel_8856\\470995937.py:5: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_value = pearsonr(data['scores'], data['ys'])\n"
     ]
    }
   ],
   "source": [
    "correlation_results = []\n",
    "# calculate the correlation between the word importance and the stock volatility\n",
    "for word, data in word_correlations.items():\n",
    "    if len(data['scores']) < 2:\n",
    "        continue\n",
    "    corr, p_value = pearsonr(data['scores'], data['ys'])\n",
    "    if p_value > 0.05: \n",
    "        continue\n",
    "    correlation_results.append( (word, corr) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = [(word, corr if not np.isnan(corr) else 0) for word, corr in correlation_results]\n",
    "correlation_results.sort(key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top influential words:\n",
      "appraisal       | Correlation: -1.000\n",
      "court           | Correlation: 1.000\n",
      "emphasis        | Correlation: -1.000\n",
      "hubandspoke     | Correlation: -1.000\n",
      "stringent       | Correlation: -1.000\n",
      "longform        | Correlation: -1.000\n",
      "broadly         | Correlation: 1.000\n",
      "reselling       | Correlation: 1.000\n",
      "achieving       | Correlation: -1.000\n",
      "month           | Correlation: -1.000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop influential words:\")\n",
    "for word, corr in correlation_results[:10]:\n",
    "    print(f\"{word:<15} | Correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(correlation_results, columns=[\"Word\", \"Correlation\"])\n",
    "df.to_csv('word importance/2000-2003result-12.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
